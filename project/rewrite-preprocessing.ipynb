{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a52bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import euclidean\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import hyperparameters as hp\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "path = hp.FLOWDATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5152daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_long_nan_sections(df, max_gap):\n",
    "    # create a mask for all of the rows with missing values\n",
    "    missing_vals = df.isna()\n",
    "    \n",
    "    prev_row_missing_vals = missing_vals.shift()\n",
    "\n",
    "    # find the rows where the value of a sensor changes from nan > value, or value > nan\n",
    "    transition_rows = missing_vals != prev_row_missing_vals\n",
    "\n",
    "    # assign an id number to each block of vals\n",
    "    block_ids = transition_rows.cumsum()\n",
    "\n",
    "    # find the length of each gap\n",
    "    gap_lengths = missing_vals.groupby(block_ids).transform('sum')\n",
    "\n",
    "    # identify all gaps which are longer than 4 hours\n",
    "    long_gaps = missing_vals & (gap_lengths > max_gap)\n",
    "\n",
    "    return long_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c250c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the flow data file\n",
    "flowdata_df = pd.read_csv(path, index_col=0)\n",
    "flowdata_df.index = pd.to_datetime(flowdata_df.index, format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Removing a sensor with a large number of missing values\n",
    "flowdata_df = flowdata_df.drop('1615', axis=1)\n",
    "\n",
    "flowdata_df = flowdata_df.rename(columns=hp.SENSOR_DMA_TO_ID)\n",
    "flowdata_df = flowdata_df.sort_index(axis=1)\n",
    "\n",
    "# Removing rows which have outliers or are part of long sections of missing values\n",
    "rows_to_remove = pd.Series(False, index=flowdata_df.index) \n",
    "for col in flowdata_df.columns:\n",
    "    rows_to_remove |= find_long_nan_sections(flowdata_df[col], hp.MAX_GAP)\n",
    "    #rows_to_remove |= find_outlier_values(flowdata_df[col])\n",
    "flowdata_df = flowdata_df[rows_to_remove == False]\n",
    "\n",
    "# Imputing short ranges of missing values\n",
    "flowdata_df = flowdata_df.interpolate(method='spline', order = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f4ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_based_train_val_test_split(flowdata_df, train_val_test_ratios):\n",
    "    # get all the months\n",
    "    df_month_strata = pd.DataFrame(index=flowdata_df.index)\n",
    "    df_month_strata['year_month'] = flowdata_df['timestamp'].dt.to_period('M')\n",
    "\n",
    "    months = df_month_strata['year_month'].unique()\n",
    "    random.shuffle(months)\n",
    "\n",
    "    # assign each month to a set\n",
    "    num_months = df_month_strata['year_month'].nunique()\n",
    "    train_ratio, val_ratio, test_ratio = train_val_test_ratios\n",
    "\n",
    "    num_months_train = int(num_months * train_ratio)\n",
    "    num_months_val = int(num_months * val_ratio)\n",
    "    num_months_test = num_months - num_months_train - num_months_val\n",
    "\n",
    "    train_months = months[:num_months_train]\n",
    "    val_months = months[num_months_train:num_months_train + num_months_val]\n",
    "    test_months = months[num_months_train + num_months_val:]\n",
    "\n",
    "    # split the data into the sets\n",
    "    train_df = flowdata_df[df_month_strata['year_month'].isin(train_months)]\n",
    "    val_df = flowdata_df[df_month_strata['year_month'].isin(val_months)]\n",
    "    test_df = flowdata_df[df_month_strata['year_month'].isin(test_months)]\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_strata(df):\n",
    "    \"\"\"\n",
    "    Takes a timeseries dataframe, and creates a corresponding series specificying each row's strata.\n",
    "    \"\"\"\n",
    "\n",
    "    strata_dict = {\n",
    "        'time_of_day': {\n",
    "            'feature_origin': df.index.hour,\n",
    "            'bins': [0, 6, 12, 14, 18, 22, 24],  \n",
    "            'labels': ['night', 'morning', 'midday', 'afternoon', 'evening', 'night']\n",
    "        },\n",
    "        'part_of_week': {\n",
    "            'feature_origin': df.index.dayofweek,\n",
    "            'bins': [0, 5, 7], \n",
    "            'labels': ['weekday', 'weekend']\n",
    "        },\n",
    "        'season': {\n",
    "            'feature_origin': df.index.month,\n",
    "            'bins': [0, 3, 6, 9, 12, 13], \n",
    "            'labels': ['winter', 'spring', 'summer', 'autumn', 'winter']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    strata_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for strata_name, strata_info in strata_dict.items():\n",
    "        strata_df[strata_name] = pd.cut(\n",
    "            strata_info['feature_origin'],\n",
    "            bins=strata_info['bins'],\n",
    "            labels=strata_info['labels'],\n",
    "            right=False,  \n",
    "            include_lowest=True,\n",
    "            ordered=False\n",
    "        )\n",
    "\n",
    "    strata_df['strata'] = strata_df['part_of_week'].astype(str) + '_' + strata_df['season'].astype(str)\n",
    "\n",
    "    return strata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05398d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(df, df_strata, sample_length, overlap):\n",
    "    \"\"\"\n",
    "    Creates samples of the data with a given length and overlap.\n",
    "\n",
    "    Note: \n",
    "    - Currently all samples will start at the same time of day, as each one is 5 days long\n",
    "    - This would make the model struggle with differently timed inputs\n",
    "    - I will need to eventually add random starting points for the samples, but for now I will just create the samples with a fixed starting point to test the model\n",
    "    \"\"\"\n",
    "\n",
    "    samples_df = pd.DataFrame(columns=hp.SENSOR_COLS)\n",
    "\n",
    "    # Find all the gaps in the data (where there are missing time steps / it is not continuous)\n",
    "    gap_mask = df.index.to_series().diff() > pd.Timedelta(minutes=15)\n",
    "\n",
    "    # Split the data into all of the continous segments\n",
    "    df['segment_id'] = gap_mask.cumsum()\n",
    "    segments_df = df.groupby('segment_id')\n",
    "\n",
    "    # Split the data into all of the continuous segments and iterate through each segment\n",
    "    for _, segment in segments_df:\n",
    "\n",
    "        # Get rid of the segment_id column\n",
    "        segment = segment.drop(columns='segment_id')\n",
    "\n",
    "        # Get the sensor values as a numpy array for easier indexing\n",
    "        sensor_values = segment[hp.SENSOR_COLS].values\n",
    "\n",
    "        if overlap:\n",
    "            step = sample_length / 2\n",
    "        else:\n",
    "            step = sample_length\n",
    "\n",
    "        i = 0\n",
    "        while i + sample_length <= len(segment):\n",
    "            row = {\n",
    "                col: sensor_values[i:i + sample_length, idx]\n",
    "                for idx, col in enumerate(hp.SENSOR_COLS)\n",
    "            }\n",
    "            index = len(samples_df)\n",
    "            samples_df.loc[index] = row\n",
    "            i += step\n",
    "    \n",
    "    return samples_df, df_strata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48088a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_random_sample(samples_df, strata_df):\n",
    "    # Merge the samples_df with the strata_df to get the strata labels for each sample\n",
    "    merged_df = samples_df.merge(strata_df, left_index=True, right_index=True)\n",
    "\n",
    "    # Perform stratified random sampling\n",
    "    sampled_df = merged_df.groupby('strata_label', group_keys=False).apply(lambda x: x.sample(hp.SAMPLE_SIZE, replace=True))\n",
    "\n",
    "    # Drop the strata_label column from the sampled dataframe\n",
    "    sampled_df = sampled_df.drop(columns='strata_label')\n",
    "\n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(flowdata_df):\n",
    "    window_size = hp.TOTAL_WINDOW\n",
    "\n",
    "    datasets = []\n",
    "    strata = []\n",
    "\n",
    "    train_df, val_df, test_df = month_based_train_val_test_split(flowdata_df, hp.TRAIN_VAL_TEST_SPLIT)\n",
    "    dfs = [train_df, val_df, test_df]\n",
    "    overlap = [True, True, False]\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        strata_df = assign_strata(df)\n",
    "        samples_df, samples_strata_df = create_samples(df, strata_df, window_size, overlap=overlap[i])\n",
    "        strat_sampled_df = strat_random_sample(samples_df, strata_df)\n",
    "        datasets.append(strat_sampled_df)\n",
    "        strata.append(strata_df)\n",
    "\n",
    "    return datasets, strata\n",
    "\n",
    "datasets, strata = preprocess_data(flowdata_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
